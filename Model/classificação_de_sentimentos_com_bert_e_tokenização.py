# -*- coding: utf-8 -*-
"""Classificação de Sentimentos com BERT e Tokenização

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u4JrA0MSDPa_OLNGZvO9RBnrUNFvNYeT

# Etapa 1: Importação das bibliotecas
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
import random
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import drive

!pip install bert-for-tf2

!pip install sentencepiece

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

#!pip install tensorflow==2.2.0-rc3
import tensorflow as tf
tf.__version__

import tensorflow_hub as hub

from tensorflow.keras import layers
import bert

"""# Etapa 2: Pré-processamento

## Carregamento da base de dados
"""

drive.mount('/content/drive')

cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']

data = pd.read_csv('/content/drive/MyDrive/Base de dados sentimentos/training.1600000.processed.noemoticon.csv',
                   header = None,
                   names = cols,
                   engine='python',
                   encoding='latin1')

data.shape

data.head()

data.drop(['id', 'date', 'query', 'user'],
          axis = 1, inplace=True)

data.shape

data.head()

data.tail()

sns.countplot(data['sentiment']);

"""## Pré-procesamento

### Limpeza dos textos
"""

def clean_tweet(tweet):
  tweet = BeautifulSoup(tweet, 'lxml').get_text()
  tweet = re.sub(r"@[A-Za-z0-9]+", ' ', tweet)
  tweet = re.sub(r"https?://[A-Za-z0-9./]+", ' ', tweet)
  tweet = re.sub(r"[^a-zA-Z.!?']", ' ', tweet)
  tweet = re.sub(r" +", ' ', tweet)
  return tweet

test = '99 ' + data.text[0]
test

result = clean_tweet(test)
result

data_clean = [clean_tweet(tweet) for tweet in data.text]

data_clean[0:4]

data_labels = data.sentiment.values
data_labels

data_labels[data_labels == 4] = 1

data_labels

"""### Tokenização"""

FullTokenizer = bert.bert_tokenization.FullTokenizer
bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1', trainable=False)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = FullTokenizer(vocab_file, do_lower_case)

vocab_file

print(tokenizer.vocab)

len(tokenizer.vocab)

tokenizer.tokenize('My dog likes strawberries.')

tokenizer.convert_tokens_to_ids(tokenizer.tokenize('My dog likes strawberries.'))

def encode_sentence(sent):
  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))

encode_sentence('my dog likes strawberries')

data_inputs = [encode_sentence(sentence) for sentence in data_clean]

data_inputs[1]

"""### Criação da base de dados"""

data_with_len = [[sent, data_labels[i], len(sent)]
                 for i, sent in enumerate(data_inputs)]

data_with_len[0:2]

random.shuffle(data_with_len)
data_with_len.sort(key=lambda x: x[2])
sorted_all = [(sent_lab[0], sent_lab[1])
              for sent_lab in data_with_len if sent_lab[2] > 7]

sorted_all[300000:300005]

all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,
                                             output_types = (tf.int32, tf.int32))

next(iter(all_dataset))

BATCH_SIZE = 32
all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))

next(iter(all_batched))

len(sorted_all)

NB_BATCHES = len(sorted_all) // BATCH_SIZE
NB_BATCHES

NB_BATCHES_TEST = NB_BATCHES // 10
NB_BATCHES_TEST

all_batched.shuffle(NB_BATCHES)
test_dataset = all_batched.take(NB_BATCHES_TEST)
train_dataset = all_batched.skip(NB_BATCHES_TEST)

next(iter(test_dataset))

next(iter(train_dataset))

"""# Etapa 3: Construção do modelo"""

class DCNN(tf.keras.Model):

  def __init__(self,
               vocab_size,
               emb_dim=128,
               nb_filters = 50,
               FFN_units=512,
               nb_classes=2,
               dropout_rate=0.1,
               training=False,
               name="dcnn"):
    super(DCNN, self).__init__(name=name)

    self.embedding = layers.Embedding(vocab_size, emb_dim)

    self.bigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 2,
                                padding='valid',
                                activation='relu')
    self.trigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 3,
                                padding='valid',
                                activation='relu')
    self.fourgram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 4,
                                padding='valid',
                                activation='relu')

    self.pool = layers.GlobalMaxPool1D()

    self.dense_1 = layers.Dense(units = FFN_units, activation='relu')
    self.dropout = layers.Dropout(rate=dropout_rate)
    if nb_classes == 2:
      self.last_dense = layers.Dense(units=1, activation='sigmoid')
    else:
      self.last_dense = layers.Dense(units=nb_classes, activation='softmax')

  def call(self, inputs, training):
    x = self.embedding(inputs)
    x_1 = self.bigram(x)
    x_1 = self.pool(x_1)
    x_2 = self.trigram(x)
    x_2 = self.pool(x_2)
    x_3 = self.fourgram(x)
    x_3 = self.pool(x_3)

    merged = tf.concat([x_1, x_2, x_3], axis = -1)
    merged = self.dense_1(merged)
    merged = self.dropout(merged, training)
    output = self.last_dense(merged)

    return output

"""# Etapa 4: Treinamento"""

VOCAB_SIZE = len(tokenizer.vocab)
EMB_DIM = 200
NB_FILTERS = 100
FFN_UNITS = 256
NB_CLASSES = 2
DROPOUT_RATE = 0.2
NB_EPOCHS = 5

Dcnn = DCNN(vocab_size=VOCAB_SIZE,
            emb_dim=EMB_DIM,
            nb_filters = NB_FILTERS,
            FFN_units = FFN_UNITS,
            nb_classes = NB_CLASSES,
            dropout_rate = DROPOUT_RATE)

if NB_CLASSES == 2:
  Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
else:
  Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])

checkpoint_path = '/content/drive/My Drive/Cursos - recursos/'

ckpt = tf.train.Checkpoint(Dcnn=Dcnn)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)

if ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  print('Latest checkpoint restored!')

class MyCustomCallBack(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    ckpt_manager.save()
    print("Checkpoint savet at {}".format(checkpoint_path))

history = Dcnn.fit(train_dataset,
                   epochs=NB_EPOCHS,
                   #steps_per_epoch = 100,
                   callbacks=[MyCustomCallBack()])

"""Epoch 1/5
  37196/Unknown - 2895s 78ms/step - loss: 0.4204 - accuracy: 0.8073Checkpoint saved at /content/drive/My Drive/Cursos - recursos/.
37196/37196 [==============================] - 2895s 78ms/step - loss: 0.4204 - accuracy: 0.8073
Epoch 2/5
37196/37196 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.8312Checkpoint saved at /content/drive/My Drive/Cursos - recursos/.
37196/37196 [==============================] - 2951s 79ms/step - loss: 0.3793 - accuracy: 0.8312
Epoch 3/5
37196/37196 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8532Checkpoint saved at /content/drive/My Drive/Cursos - recursos/.
37196/37196 [==============================] - 2904s 78ms/step - loss: 0.3378 - accuracy: 0.8532
Epoch 4/5
37196/37196 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8735Checkpoint saved at /content/drive/My Drive/Cursos - recursos/.
37196/37196 [==============================] - 2932s 79ms/step - loss: 0.2964 - accuracy: 0.8735
Epoch 5/5
37196/37196 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.8904Checkpoint saved at /content/drive/My Drive/Cursos - recursos/.
37196/37196 [==============================] - 2858s 77ms/step - loss: 0.2588 - accuracy: 0.8904

# Etapa 5: Avaliação
"""

history.history.keys()

plt.plot(history.history['loss'])
plt.title('Loss progress');

plt.plot(history.history['accuracy'])
plt.title('Accuracy progress');

results = Dcnn.evaluate(test_dataset)
print(results)

def get_prediction(sentence):
  tokens = encode_sentence(sentence)
  inputs = tf.expand_dims(tokens, 0) # (batch_size) (1,...)

  output = Dcnn(inputs, training=False)

  sentiment = math.floor(output*2)

  if sentiment == 0:
    print('negative')
  elif sentiment == 1:
    print('positive')

get_prediction('This movie was pretty interesting')

get_prediction("I'd rather not do that again")
