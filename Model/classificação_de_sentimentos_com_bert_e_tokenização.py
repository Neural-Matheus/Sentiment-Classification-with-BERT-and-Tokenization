# -*- coding: utf-8 -*-
"""Classificação de Sentimentos com BERT e Tokenização.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sijkk17ABF9IudVkZqFwwibQOdtcty-m

# Etapa 1: Importação das Bibliotecas
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
import random
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import drive

!pip install bert-for-tf2

!pip install sentencepiece

import tensorflow as tf

tf.__version__

import tensorflow_hub as hub

from tensorflow.keras import layers
import bert

"""# Etapa 2: Pré-processamento

## Carregamento da base de dados
"""

drive.mount('/content/drive/')

cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']

data = pd.read_csv('/content/drive/MyDrive/Base de dados sentimentos/training.1600000.processed.noemoticon.csv',
                   header = None,
                   names = cols,
                   engine = 'python',
                   encoding = 'latin1')

data.shape

data.head()

data.drop(['id', 'date', 'query', 'user'],
          axis = 1,
          inplace = True)

data.shape

data.head()

data.tail()

sns.countplot(data['sentiment']);

"""## Limpeza dos textos

"""

def clean_tweet(tweet):
    # Remover menções a usuários
    tweet = re.sub(r"@[A-Za-z0-9_]+", '', tweet)
    # Remover URLs
    tweet = re.sub(r"https?://[A-Za-z0-9./]+", '', tweet)
    # Manter apenas letras, pontos de exclamação, interrogação e ponto final
    tweet = re.sub(r"[^A-Za-z.!?]", ' ', tweet)
    # Remover espaços extras
    tweet = re.sub(r" +", ' ', tweet)
    return tweet

test = '99 ' + data.text[0]

test

result = clean_tweet(test)
result

data_clean = [clean_tweet(tweet) for tweet in data.text]

data_clean[0:4]

data_labels = data.sentiment.values
data_labels

data_labels[data_labels == 4] = 1

data_labels

"""## Tokenização

"""

FullTokenizer = bert.bert_tokenization.FullTokenizer
bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1', trainable=False)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = FullTokenizer(vocab_file, do_lower_case)

vocab_file

print(tokenizer.vocab)

len(tokenizer.vocab)

tokenizer.tokenize('my')

tokenizer.convert_tokens_to_ids(tokenizer.tokenize('I love before'))

def enconde_sentence(sent):
  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))

enconde_sentence("I love before")

data_inputs = [enconde_sentence(sentence) for sentence in data_clean]

data_inputs[0]

data_with_len = [[sent, data_labels[i], len(sent)]
                 for i, sent in enumerate(data_inputs)]

data_with_len[0:2]

random.shuffle(data_with_len)
data_with_len.sort(key = lambda x: x [2])
sorted_all = [(sent_lab[0], sent_lab[1])
              for sent_lab in data_with_len if sent_lab[2] > 7
              ]

sorted_all[0:5]

all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,
                                             output_types = (tf.int32, tf.int32))

next(iter(all_dataset))

BATCH_SIZE = 32
all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,), ()))

next(iter(all_batched))

len(sorted_all)

NB_BATCHES = len(sorted_all) // BATCH_SIZE
NB_BATCHES

NB_BATCHES_TEST = NB_BATCHES // 10
NB_BATCHES_TEST

all_batched.shuffle(NB_BATCHES)

test_dataset = all_batched.take(NB_BATCHES_TEST)
train_dataset = all_batched.skip(NB_BATCHES_TEST)

next(iter(test_dataset))

next(iter(train_dataset))

"""# Etapa 3: Construção do Modelo

"""

class DCNN(tf.keras.Model):

  def __init__(self,
               vocab_size,
               emb_din = 128,
               nb_filters = 50,
               FFN_units = 512,
               nb_classes = 2,
               dropout_rate = 0.1,
               training = False,
               name = 'dcnn'):

    super(DCNN, self).__init__(name = name)

    self.embedding = layer.Embedding(vocab_size, emb_dim)

    self.bigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 2,
                                padding = 'valid',
                                activation = 'relu')

    self.trigram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 3,
                                padding = 'valid',
                                activation = 'relu')

    self.fourgram = layers.Conv1D(filters = nb_filters,
                                kernel_size = 4,
                                padding = 'valid',
                                activation = 'relu')

    self.pool = layers.GlobalMaxPool1D()

    self.dense_1 = layers.Dense(units = FFN_units, activation = 'relu')
    self.dropout = layers.Dropout(rate = dropout_rate)

    if nb_classes == 2:
      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')
    else:
      self.last_dense = layers.Dense(units = nb_classes, activation = 'softmax')

  def call(self, inputs, training):
    x = self.embedding(inputs)
    x_1 = self.bigram(x)
    X_1 = self.pool(x_1)

    x_2 = self.trigram(x)
    X_2 = self.pool(x_2)

    x_3 = self.fourgram(x)
    X_3 = self.pool(x_3)

    merged = tf.concat([x_1, x_2, x_3], axis = -1)
    merged = self.dense_1(merged)
    merged = self.dropout(merged, training)
    output = self.last_dense(merged)

    return output